{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Transformer lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.5006],\n",
      "        [0.2748]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1603, -0.4229, -0.9776],\n",
      "        [ 0.1618, -0.6182, -1.1653],\n",
      "        [-0.0393, -1.2973,  3.2234],\n",
      "        [ 1.1670, -2.0040,  0.2367],\n",
      "        [ 0.1582,  0.9344,  1.2147],\n",
      "        [ 0.7320,  1.2849, -0.3356],\n",
      "        [-0.3271, -0.3789, -0.0440],\n",
      "        [-0.9420,  0.6211, -0.1851],\n",
      "        [-0.7103,  1.6472, -0.8887],\n",
      "        [ 1.8583, -1.1187, -1.3211]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# how to initialize neurons,\n",
    "# how to initialize embedding matrix\n",
    "\n",
    "neurons = nn.Linear(1,2)\n",
    "\n",
    "embedding = nn.Embedding(10, 3)\n",
    "\n",
    "\n",
    "print(neurons.weight)\n",
    "print(embedding.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE INPUTS \n",
      " tensor([[9],\n",
      "        [0]])\n",
      "THE EMBEDDING MATRIX \n",
      " Parameter containing:\n",
      "tensor([[-0.0405,  0.1501, -0.3983],\n",
      "        [ 0.0072, -1.7136,  0.6234],\n",
      "        [-0.0432,  0.1452, -1.0400],\n",
      "        [ 0.3987,  0.0086,  0.5366],\n",
      "        [ 0.3724,  1.1028, -0.5192],\n",
      "        [ 0.2357, -0.9623, -0.8102],\n",
      "        [-0.1859,  0.2538,  0.6283],\n",
      "        [ 0.9312, -1.4137,  1.2699],\n",
      "        [ 1.1944, -2.5325, -1.3760],\n",
      "        [ 0.6366, -0.2231,  0.4830]], requires_grad=True)\n",
      "EMBEDDINGS FOR THE GIVEN INPUTS \n",
      " tensor([[[ 0.6366, -0.2231,  0.4830]],\n",
      "\n",
      "        [[-0.0405,  0.1501, -0.3983]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# an Embedding matrix of 10 embeddings with 3 dimensions each\n",
    "embedding = nn.Embedding(10, 3)\n",
    "\n",
    "# a batch of 2 samples of 1 index each\n",
    "input = torch.LongTensor([[9],[0]])\n",
    "\n",
    "print('THE INPUTS \\n',input)\n",
    "\n",
    "# what does the embedding matrix look like?\n",
    "print('THE EMBEDDING MATRIX \\n',embedding.weight)\n",
    "\n",
    "# what is the result of the input embedding?\n",
    "print('EMBEDDINGS FOR THE GIVEN INPUTS \\n',embedding(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Embedding \n",
    "\n",
    "the embedding module creates a lookup table with a configurable number of rows and columns. \n",
    "- number of rows is the number of unique words in the vocabulary,\n",
    "- columns is the size of the word embeddings.\n",
    "\n",
    " The embedding module is initialized with random values, and the embeddings are learned during training.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt','r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the text is  1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print('the length of the text is ',len(text))\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, lets tokenize this bih\n",
    "\n",
    "embedding layer is a lookup table, \n",
    "\n",
    "- input to the embedding layer is a list of indices\n",
    "- output is a list of word (token) embeddings.\n",
    "\n",
    "lets turn our dataset into a list of indices, and feed it to the embedding layer. \n",
    "\n",
    "#### how do we do this?\n",
    "(We're doing a character level model)\n",
    "\n",
    "[x] we find all possible vocab words (letters) in datataset \n",
    "    - separate the dataset by letters, make a set of them, a list \n",
    "- tokenize the dataset wrt the vocab \n",
    "    - enumerate every element in the list \n",
    "    - make a function that returns index num for each string\n",
    "        - and a string for each num\n",
    "- pass all of these token indices into the embedding table\n",
    "    - turn entire dataset into nums wrt this tokenization strat\n",
    "- get back all the embeddings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# FIND ALL POSSIBLE VOCAB WORDS IN DATASET \n",
    "\n",
    "vocab_letters  = sorted(list(set(text)))\n",
    "print(''.join(vocab_letters))\n",
    "print(len(vocab_letters))\n",
    "\n",
    "# this is the entirety of our vocab for our word level transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAKE A FUNCTION THAT ENCODES STR -> IDX AND\n",
    "\n",
    " DECODES  IDX -> BACK TO STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is our dictionary of letters and their indices: {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "does our encoder work?  [39, 51, 53, 52, 45, 59, 57, 1, 40, 39, 40, 63]\n",
      "our dataset turned into numbers: [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59]\n"
     ]
    }
   ],
   "source": [
    "# INDEX ALL OF THE LETTERS\n",
    "# LETTER ENCODER\n",
    "\n",
    "# maps between the index and the letter \n",
    "letter_list = {letter:index for index, letter in enumerate(vocab_letters)}\n",
    "\n",
    "print('here is our dictionary of letters and their indices:',letter_list)\n",
    "\n",
    "str_to_int = lambda s: [letter_list[letter] for letter in s]\n",
    "\n",
    "print('does our encoder work? ',str_to_int('amongus baby'))\n",
    "\n",
    "numbered_dataset = str_to_int(text)\n",
    "\n",
    "# here is the dataset as a list of numbers\n",
    "print('our dataset turned into numbers:', numbered_dataset[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "DECODED MESSAGE  amongus baby\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# DECODER - ints back to str\n",
    "# jsut in case \n",
    "\n",
    "int_to_str = { index: letter for index, letter in enumerate(vocab_letters)}\n",
    "\n",
    "# here is the mapping of the numbers back to letters \n",
    "print(int_to_str)\n",
    "\n",
    "decoded = lambda i : ''.join([int_to_str[integer] for integer in i ])\n",
    "\n",
    "# lets make sure the decoder works \n",
    "print('DECODED MESSAGE ',decoded([39, 51, 53, 52, 45, 59, 57, 1, 40, 39, 40, 63]))\n",
    "\n",
    "decoded_dataset = decoded(numbered_dataset)\n",
    "\n",
    "print(decoded_dataset[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets make an embedding matrix \n",
    "\n",
    "where each character has its own embedding vector \n",
    "lets make the vectors of depth 1000!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "vocab_length = len(vocab_letters)\n",
    "\n",
    "embedding_matrix = nn.Embedding(vocab_length, 5, sparse=True)\n",
    "\n",
    "# ok so now every single element of vocab has a respective embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the encoded word: [39, 51, 53, 52, 45, 59, 57, 57, 63]\n",
      "this is the word embedding: \n",
      " tensor([[ 4.4324e-01,  1.0784e+00, -3.3171e-02, -1.9377e+00,  5.2010e-01],\n",
      "        [ 3.8638e-01, -6.0041e-01,  4.1063e-01, -1.5800e-01, -3.6941e-01],\n",
      "        [ 1.3314e+00,  1.4940e-01, -1.0414e+00, -7.3719e-01, -3.5982e-01],\n",
      "        [-8.3868e-01, -6.4568e-01,  1.5569e-01, -2.0627e-02, -1.1563e+00],\n",
      "        [-1.6021e-03,  1.5700e+00,  1.1495e+00, -1.1201e+00,  1.4911e+00],\n",
      "        [-1.2403e+00, -1.4496e+00, -1.7733e-01, -3.7104e-02,  1.9600e+00],\n",
      "        [ 8.7598e-01, -9.4798e-01, -8.7764e-02,  6.6826e-02,  4.0936e-01],\n",
      "        [ 8.7598e-01, -9.4798e-01, -8.7764e-02,  6.6826e-02,  4.0936e-01],\n",
      "        [-7.0437e-02,  2.4551e-01, -1.5056e+00, -2.5242e+00,  4.6545e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# quick example \n",
    "\n",
    "# lets tokenize a word and embbed it \n",
    "\n",
    "word = 'amongussy'\n",
    "\n",
    "encoded_word = str_to_int(word)\n",
    "\n",
    "print('this is the encoded word:' ,encoded_word)\n",
    "\n",
    "# get the embeddings for each one of these tokens \n",
    "# turn list into a tensor (basically the same as a list)\n",
    "encoded_word_tensor = torch.LongTensor(encoded_word)\n",
    "\n",
    "embedded_word = embedding_matrix(encoded_word_tensor)\n",
    "\n",
    "print('this is the word embedding: \\n', embedded_word)\n",
    "\n",
    "# basically we are getting the index of each one of those elements and retrieving the embedding from  it\n",
    "# reminded --> the embedding class starts as a random matrix and converges to true, meaning rich emebddings for each word!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets encode the whole damn dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first thousand elements of the dataset  [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59, 1, 39, 56, 43, 1, 39, 50, 50, 1, 56, 43, 57, 53, 50, 60, 43, 42, 1, 56, 39, 58, 46, 43, 56, 1, 58, 53, 1, 42, 47, 43, 1, 58, 46, 39, 52, 1, 58, 53, 1, 44, 39, 51, 47, 57, 46, 12, 0, 0, 13, 50, 50, 10, 0, 30, 43, 57, 53, 50, 60, 43, 42, 8, 1, 56, 43, 57, 53, 50, 60, 43, 42, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 18, 47, 56, 57, 58, 6, 1, 63, 53, 59, 1, 49, 52, 53, 61, 1, 15, 39, 47, 59, 57, 1, 25, 39, 56, 41, 47, 59, 57, 1, 47, 57, 1, 41, 46, 47, 43, 44, 1, 43, 52, 43, 51, 63, 1, 58, 53, 1, 58, 46, 43, 1, 54, 43, 53, 54, 50, 43, 8, 0, 0, 13, 50, 50, 10, 0, 35, 43, 1, 49, 52, 53, 61, 5, 58, 6, 1, 61, 43, 1, 49, 52, 53, 61, 5, 58, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 24, 43, 58, 1, 59, 57, 1, 49, 47, 50, 50, 1, 46, 47, 51, 6, 1, 39, 52, 42, 1, 61, 43, 5, 50, 50, 1, 46, 39, 60, 43, 1, 41, 53, 56, 52, 1, 39, 58, 1, 53, 59, 56, 1, 53, 61, 52, 1, 54, 56, 47, 41, 43, 8, 0, 21, 57, 5, 58, 1, 39, 1, 60, 43, 56, 42, 47, 41, 58, 12, 0, 0, 13, 50, 50, 10, 0, 26, 53, 1, 51, 53, 56, 43, 1, 58, 39, 50, 49, 47, 52, 45, 1, 53, 52, 5, 58, 11, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 1, 42, 53, 52, 43, 10, 1, 39, 61, 39, 63, 6, 1, 39, 61, 39, 63, 2, 0, 0, 31, 43, 41, 53, 52, 42, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 27, 52, 43, 1, 61, 53, 56, 42, 6, 1, 45, 53, 53, 42, 1, 41, 47, 58, 47, 64, 43, 52, 57, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 35, 43, 1, 39, 56, 43, 1, 39, 41, 41, 53, 59, 52, 58, 43, 42, 1, 54, 53, 53, 56, 1, 41, 47, 58, 47, 64, 43, 52, 57, 6, 1, 58, 46, 43, 1, 54, 39, 58, 56, 47, 41, 47, 39, 52, 57, 1, 45, 53, 53, 42, 8, 0, 35, 46, 39, 58, 1, 39, 59, 58, 46, 53, 56, 47, 58, 63, 1, 57, 59, 56, 44, 43, 47, 58, 57, 1, 53, 52, 1, 61, 53, 59, 50, 42, 1, 56, 43, 50, 47, 43, 60, 43, 1, 59, 57, 10, 1, 47, 44, 1, 58, 46, 43, 63, 0, 61, 53, 59, 50, 42, 1, 63, 47, 43, 50, 42, 1, 59, 57, 1, 40, 59, 58, 1, 58, 46, 43, 1, 57, 59, 54, 43, 56, 44, 50, 59, 47, 58, 63, 6, 1, 61, 46, 47, 50, 43, 1, 47, 58, 1, 61, 43, 56, 43, 0, 61, 46, 53, 50, 43, 57, 53, 51, 43, 6, 1, 61, 43, 1, 51, 47, 45, 46, 58, 1, 45, 59, 43, 57, 57, 1, 58, 46, 43, 63, 1, 56, 43, 50, 47, 43, 60, 43, 42, 1, 59, 57, 1, 46, 59, 51, 39, 52, 43, 50, 63, 11, 0, 40, 59, 58, 1, 58, 46, 43, 63, 1, 58, 46, 47, 52, 49, 1, 61, 43, 1, 39, 56, 43, 1, 58, 53, 53, 1, 42, 43, 39, 56, 10, 1, 58, 46, 43, 1, 50, 43, 39, 52, 52, 43, 57, 57, 1, 58, 46, 39, 58, 0, 39, 44, 44, 50, 47, 41, 58, 57, 1, 59, 57, 6, 1, 58, 46, 43, 1, 53, 40, 48, 43, 41, 58, 1, 53, 44, 1, 53, 59, 56, 1, 51, 47, 57, 43, 56, 63, 6, 1, 47, 57, 1, 39, 57, 1, 39, 52, 0, 47, 52, 60, 43, 52, 58, 53, 56, 63, 1, 58, 53, 1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47, 57, 43, 1, 58, 46, 43, 47, 56, 1, 39, 40, 59, 52, 42, 39, 52, 41, 43, 11, 1, 53, 59, 56, 0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43, 1, 47, 57, 1, 39, 1, 45, 39, 47, 52, 1, 58, 53, 1, 58, 46, 43, 51, 1, 24, 43, 58, 1, 59, 57, 1, 56, 43, 60, 43, 52, 45, 43, 1, 58, 46, 47, 57, 1, 61, 47, 58, 46, 0, 53, 59, 56, 1, 54, 47, 49, 43, 57, 6, 1, 43, 56, 43, 1, 61, 43, 1, 40, 43, 41, 53, 51, 43, 1, 56, 39, 49, 43, 57, 10, 1, 44, 53, 56, 1, 58, 46, 43, 1, 45, 53, 42, 57, 1, 49, 52, 53, 61, 1, 21, 0, 57, 54, 43, 39, 49, 1, 58, 46, 47, 57, 1, 47, 52, 1, 46, 59, 52, 45, 43, 56, 1, 44, 53, 56, 1, 40, 56, 43, 39, 42, 6, 1, 52, 53, 58, 1, 47, 52, 1, 58, 46, 47, 56, 57, 58, 1, 44, 53, 56, 1, 56, 43, 60, 43, 52, 45, 43, 8, 0, 0]\n",
      "tensor([18, 47, 56,  ..., 45,  8,  0])\n",
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# lets encode the whole damn dataset \n",
    "\n",
    "encoded_dataset = str_to_int(text)\n",
    "\n",
    "print('first thousand elements of the dataset ',encoded_dataset[:1000])\n",
    "\n",
    "data = torch.tensor(encoded_dataset, dtype=torch.long)\n",
    "\n",
    "print(data)\n",
    "\n",
    "# pprint('this is the encoded dataset \\n', str(encoded_dataset))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like thisb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1981, -0.2763,  0.2850,  0.0373,  0.8419],\n",
       "        [-0.6436, -1.6836, -0.6482,  0.6465,  0.6399],\n",
       "        [-0.6567,  0.7852,  0.3708, -0.4710,  0.2444],\n",
       "        [ 0.8760, -0.9480, -0.0878,  0.0668,  0.4094],\n",
       "        [ 1.7495, -0.8827,  0.7772,  0.3706, -0.0818],\n",
       "        [-1.1026,  0.3207,  1.4355,  1.2953, -0.1130],\n",
       "        [-0.8380, -0.7666, -0.0140, -1.8904, -0.7230],\n",
       "        [-0.6436, -1.6836, -0.6482,  0.6465,  0.6399],\n",
       "        [ 1.7495, -0.8827,  0.7772,  0.3706, -0.0818],\n",
       "        [-0.6436, -1.6836, -0.6482,  0.6465,  0.6399],\n",
       "        [-0.3891, -0.3347,  0.5705, -1.0866, -0.1941],\n",
       "        [ 0.5354,  0.1426,  0.3261,  0.1197, -0.6784],\n",
       "        [-0.8387, -0.6457,  0.1557, -0.0206, -1.1563],\n",
       "        [-0.3537,  0.4702,  0.7982, -0.4346, -2.1039],\n",
       "        [ 0.1819, -0.4138,  0.1301, -0.2231, -0.6040],\n",
       "        [-0.8845, -1.8248, -0.4857,  0.6976,  0.8490]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets retrieve the embeddings for the first 8 elements of the dataset \n",
    "\n",
    "sequence_len = 16\n",
    "\n",
    "# this pulls sequence_len amount of embeddings out of the matrix from the dataset indexes \n",
    "# the rows are the embeddings and columns are the features \n",
    "embedding_matrix(data[:sequence_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embbed the whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394, 5])"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_dataset= embedding_matrix(data)\n",
    "\n",
    "# now each letter has its own embedding vector \n",
    "embedded_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset is embedded!!\n",
    "\n",
    "pass into transformer block\n",
    "\n",
    "what's in the **block**??\n",
    "\n",
    "#### 1. attention head \n",
    "#### 2. feedforward layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we gotta figure out how to do a foward pass\n",
    "- what does the forward pass do??\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch that\n",
    "\n",
    "lets implement a head of attention first, then we can put this into the model\n",
    "- why? bc i felt like it lel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Attention\n",
    "#### What does a head of attention do??\n",
    "\n",
    "it creates \n",
    "1. queries ‚ùì\n",
    "2. keys üîë\n",
    "3. values üó£Ô∏è\n",
    "\n",
    "we multiply k and q to get the affinities/attention scores between embeddings\n",
    "\n",
    "softmax attention scores to get weights\n",
    "\n",
    "mult weights & values to get the ŒîEmbedding\n",
    "\n",
    "add ŒîEmbedding to Embeddings recursively \n",
    "- slowly add more and more semantic meaning to embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Actually jk\n",
    "\n",
    "lets get this thing to create completions.\n",
    "\n",
    "even if they suck, lets just get it to work first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING MATRIX \n",
      " Embedding(65, 65)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [ 1.3035, -0.4501,  1.3471,  ...,  0.1910, -0.3425,  1.7955],\n",
      "        ...,\n",
      "        [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
      "        [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064],\n",
      "        [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]],\n",
      "       requires_grad=True)\n",
      "---------------------------\n",
      "EMBEDDING THE DATA (first 1000) \n",
      " torch.Size([1000, 65])\n",
      "tensor([[-0.1935, -0.6324, -0.2059,  ..., -0.5336, -1.3621,  0.3269],\n",
      "        [ 1.6515, -0.0424, -0.7355,  ...,  0.8682,  2.0593, -0.8159],\n",
      "        [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
      "        ...,\n",
      "        [ 1.1407,  0.8935, -2.4000,  ...,  0.3227,  1.5431, -1.0392],\n",
      "        [ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
      "        [ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#  the bigram learns probabilities of the next token based on the previous one \n",
    "\n",
    "# the model's forward function gets called when you call the model\n",
    "#  this is part of the nn.Module functionality \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size): \n",
    "        # initialize the nn.Module parent class\n",
    "        super().__init__()\n",
    "        # initialize the embedding table for every element in the vocab\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # get the logits of the last character \n",
    "        # measure how far off we where from our targets\n",
    "        embeddings = self.token_embedding_table(idx) # --> returns indexes + embeddings \n",
    "        # basically (B,T, C) adding the channel, C dimension\n",
    "        return embeddings\n",
    "\n",
    "model = BigramLanguageModel(vocab_length)\n",
    "\n",
    "# INITALIZED EMBEDDING table \n",
    "token_embedding_table = model.token_embedding_table\n",
    "print('EMBEDDING MATRIX \\n',token_embedding_table)\n",
    "print(token_embedding_table.weight)\n",
    "\n",
    "print('---------------------------')\n",
    "\n",
    "# embed first 1000 elements of data, making them 65-dimensional vectors \n",
    "embedding_tensor = model.forward(data[:1000])\n",
    "print('EMBEDDING THE DATA (first 1000) \\n', embedding_tensor.shape)\n",
    "print(embedding_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick little recap of wtf we just did \n",
    "\n",
    "so we started a bigram language model \n",
    "\n",
    "we initalized the super() parent module nn.Module \n",
    "\n",
    "we created an embedding table of vocab_size by n-dimensions\n",
    "\n",
    "we created the forward function with limited functionality  \n",
    "- right now all it does is retrieve the vector embeddings for the indices (idx) that are passed into it \n",
    "- returning a B, T, C tensor where T is sequence length and C is the channels of the embeddings \n",
    "\n",
    "#### Optimization\n",
    "\n",
    "now- we dont want to run the entire sequence linearly \n",
    "- we can strip out the sequence into batches \n",
    "    - this allows us to run many batches n parallel\n",
    "    - parallelism is our friend - GPUs can compute many things simultaneously\n",
    "        - we want to encourage these capabilities in our program  \n",
    "    - we can do many sequences in parallel instead of one linearly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets understand the feedforward mechanism \n",
    "\n",
    "sorry i cant do this one from first principles\n",
    "\n",
    "that shit is hard to keep all in mind simultaneously\n",
    "\n",
    "i know a few things \n",
    "- this is a classic neural net\n",
    "    - has weights, biases\n",
    "    - and non-linear layers \n",
    "        - the non-linearity allow for the model to inch towards different weights to optimize itself with respect to a goal \n",
    "        - imagine a machine that retrieves a delta between a ball throw and the target it wanted, the non-linearity allows the machine to adjust the force (wweight) of the throw with respect to the target (loss)\n",
    "\n",
    "so we'll just copy paste it and break it down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 65])\n",
      "tensor([[-0.1063,  0.0056,  0.0715,  ...,  0.2231, -0.0992,  0.1521],\n",
      "        [-0.1040,  0.2804,  0.0873,  ..., -0.2492, -0.1800,  0.1106],\n",
      "        [ 0.0151, -0.1253,  0.2796,  ..., -0.2357, -0.0340, -0.0983],\n",
      "        ...,\n",
      "        [ 0.1486, -0.1871, -0.1081,  ..., -0.1200, -0.3288, -0.0159],\n",
      "        [ 0.1479, -0.1909, -0.0126,  ...,  0.0750, -0.0729,  0.1843],\n",
      "        [ 0.1479, -0.1909, -0.0126,  ...,  0.0750, -0.0729,  0.1843]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Linear(in_features=65, out_features=260, bias=True)\n"
     ]
    }
   ],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# here, n_embd is the number of dimensions in the embedding\n",
    "# the first linear layer projects the embeddings to a higher-dimensional space\n",
    "# the second linear layer brings it back down to the original dimensionality\n",
    "# the ReLU activation function is applied in between the two linear layers\n",
    "\n",
    "ffw = FeedFoward(65).forward(embedding_tensor)\n",
    "\n",
    "print(ffw.shape)\n",
    "print(ffw)\n",
    "\n",
    "n_embd = 65\n",
    "\n",
    "print(nn.Linear(n_embd, 4 * n_embd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes on the FeedFoward layer\n",
    "\n",
    "so the linear layer\n",
    "- n_embed is the size/dimensions of the embedding vectors (# of features/token)\n",
    "projects the input to a higher-dimensional space, then back down\n",
    "This allows for more complex transformations of the data\n",
    "- the expansion and contraction create a \"bottleneck\" architecture\n",
    "\n",
    "Process flow:\n",
    "\n",
    "Input (65 dimensions) -> Expand to 260 -> Apply ReLU -> Reduce back to 65\n",
    "\n",
    "\n",
    "Physical interpretation:\n",
    "\n",
    "- Each of the 65 input \"neurons\" connects to multiple neurons in the 260-dimensional space\n",
    "- It's not exactly \"each neuron has four output dimensions\", but rather a full connection where each of the 65 inputs influences all 260 outputs\n",
    "- ReLU then acts on these 260 values, potentially setting some to zero\n",
    "- The 260 values are then combined in various ways to produce the 65 output values\n",
    "\n",
    "\n",
    "eedForward layer expands the input to a higher dimension, applies non-linearity, and then projects it back to the original dimension, all while maintaining the sequence length.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "basically what is happening here is that the C param of each (B, T, C) is being multiplied\n",
    "- higher embedding vector length \n",
    "- creates a richer area to optimize/apply relu/bias onto \n",
    "\n",
    "When an `nn.Linear` object is created, it randomly initializes a weight matrix and a bias vector. The size of the weight matrix is `out_features` x `in_features`, and the size of the bias vector is `out_features`.\n",
    "\n",
    "another thing that this linear layuer is doing\n",
    "turning ine output into multiple outputs, how?\n",
    "- weighting that one output out into a sum of one across all the output projections \n",
    "- adding a bias/relu \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n",
      "torch.Size([4, 6])\n",
      "tensor([[-4.2169e-01,  2.0878e+00,  1.0460e-01, -2.3480e+00, -1.5604e+00],\n",
      "        [ 4.2957e-01,  7.5407e-01, -1.4370e+00, -6.0126e-01, -4.5125e-01],\n",
      "        [ 8.6857e-01,  7.9275e-01,  6.0557e-01,  2.3086e-04,  7.8297e-03],\n",
      "        [-1.2700e+00,  5.9990e-01, -7.4589e-01,  3.0727e-01, -7.2420e-01]])\n",
      "tensor([[-0.9884, -1.4970, -0.5441,  2.3051, -0.8877,  2.0593],\n",
      "        [-0.3017,  0.0825, -0.3103,  0.4518, -1.0844,  0.8471],\n",
      "        [-0.7126,  0.0435,  1.0352,  0.1520, -0.6396, -0.1819],\n",
      "        [ 0.1801,  0.2077, -0.4750,  0.3845, -0.4263,  0.9890]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Parameter containing:\n",
      "tensor([-0.2998,  0.1982,  0.1845, -0.0683, -0.3465,  0.0981],\n",
      "       requires_grad=True)\n",
      "3.5410000000000004\n",
      "-1.0550000000000002\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(5, 6)\n",
    "\n",
    "input = torch.randn(4, 5)\n",
    "\n",
    "output = m(input)\n",
    "\n",
    "print(input.shape)\n",
    "print(output.shape)\n",
    "\n",
    "print(input)\n",
    "print(output)\n",
    "\n",
    "print(m.bias)\n",
    "\n",
    "print(sum([ 0.9070,  2.0752, -0.2229, -0.4429,  1.2246]))\n",
    "print(sum([-0.4813,  0.4688, -0.8751, -1.2857,  0.1232,  0.9951]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
