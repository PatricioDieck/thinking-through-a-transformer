{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Transformer lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1533],\n",
      "        [-0.9457]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "neurons = nn.Linear(1,2)\n",
    "\n",
    "embedding = nn.Embedding(10, 3)\n",
    "\n",
    "print(neurons.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE INPUTS \n",
      " tensor([[9],\n",
      "        [0]])\n",
      "THE EMBEDDING MATRIX \n",
      " Parameter containing:\n",
      "tensor([[-0.8222, -0.4863,  1.7287],\n",
      "        [ 0.4299, -0.1073,  0.3182],\n",
      "        [-0.8278,  0.4542,  1.5049],\n",
      "        [ 0.7230,  0.9364, -1.5144],\n",
      "        [ 0.8682,  0.6140,  0.4117],\n",
      "        [-1.3442,  1.0153,  1.1109],\n",
      "        [ 1.1355,  0.6441, -1.2329],\n",
      "        [-0.2634,  0.9824, -0.2587],\n",
      "        [ 1.1496, -0.6391, -0.4868],\n",
      "        [-0.3935,  1.1036, -0.4481]], requires_grad=True)\n",
      "EMBEDDINGS FOR THE GIVEN INPUTS \n",
      " tensor([[[-0.3935,  1.1036, -0.4481]],\n",
      "\n",
      "        [[-0.8222, -0.4863,  1.7287]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# an Embedding matrix of 10 embeddings with 3 dimensions each\n",
    "embedding = nn.Embedding(10, 3)\n",
    "\n",
    "# a batch of 2 samples of 1 index each\n",
    "input = torch.LongTensor([[9],[0]])\n",
    "\n",
    "print('THE INPUTS \\n',input)\n",
    "\n",
    "# what does the embedding matrix look like?\n",
    "print('THE EMBEDDING MATRIX \\n',embedding.weight)\n",
    "\n",
    "# what is the result of the input embedding?\n",
    "print('EMBEDDINGS FOR THE GIVEN INPUTS \\n',embedding(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Embedding \n",
    "\n",
    "the embedding module creates a lookup table with a configurable number of rows and columns. \n",
    "- number of rows is the number of unique words in the vocabulary,\n",
    "- columns is the size of the word embeddings.\n",
    "\n",
    " The embedding module is initialized with random values, and the embeddings are learned during training.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt','r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the text is  1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print('the length of the text is ',len(text))\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, lets tokenize this bih\n",
    "\n",
    "embedding layer is a lookup table, \n",
    "\n",
    "- input to the embedding layer is a list of indices\n",
    "- output is a list of word (token) embeddings.\n",
    "\n",
    "lets turn our dataset into a list of indices, and feed it to the embedding layer. \n",
    "\n",
    "#### how do we do this?\n",
    "(We're doing a character level model)\n",
    "\n",
    "[x] we find all possible vocab words (letters) in datataset \n",
    "    - separate the dataset by letters, make a set of them, a list \n",
    "- tokenize the dataset wrt the vocab \n",
    "    - enumerate every element in the list \n",
    "    - make a function that returns index num for each string\n",
    "        - and a string for each num\n",
    "- pass all of these token indices into the embedding table\n",
    "    - turn entire dataset into nums wrt this tokenization strat\n",
    "- get back all the embeddings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# FIND ALL POSSIBLE VOCAB WORDS IN DATASET \n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "vocab_letters  = sorted(list(set(text)))\n",
    "print(''.join(vocab_letters))\n",
    "print(len(vocab_letters))\n",
    "\n",
    "# this is the entirety of our vocab for our word level transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAKE A FUNCTION THAT ENCODES STR -> IDX AND\n",
    "\n",
    " DECODES  IDX -> BACK TO STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does our encoder work?  [39, 51, 53, 52, 45, 59, 57, 1, 40, 39, 40, 63]\n",
      "our dataset turned into numbers: [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59]\n"
     ]
    }
   ],
   "source": [
    "# INDEX ALL OF THE LETTERS\n",
    "# LETTER ENCODER\n",
    "\n",
    "# maps between the index and the letter \n",
    "letter_list = {letter:index for index, letter in enumerate(vocab_letters)}\n",
    "\n",
    "str_to_int = lambda s: [letter_list[letter] for letter in s]\n",
    "\n",
    "print('does our encoder work? ',str_to_int('amongus baby'))\n",
    "\n",
    "numbered_dataset = str_to_int(text)\n",
    "\n",
    "# here is the dataset as a list of numbers\n",
    "print('our dataset turned into numbers:', numbered_dataset[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# DECODER - ints back to str\n",
    "# jsut in case \n",
    "\n",
    "int_to_str = { index: letter for index, letter in enumerate(vocab_letters)}\n",
    "\n",
    "# here is the mapping of the numbers back to letters \n",
    "print(int_to_str)\n",
    "\n",
    "decoded = lambda i : ''.join([int_to_str[integer] for integer in i ])\n",
    "\n",
    "# lets make sure the decoder works \n",
    "decoded([39, 51, 53, 52, 45, 59, 57, 1, 40, 39, 40, 63])\n",
    "\n",
    "decoded_dataset = decoded(numbered_dataset)\n",
    "\n",
    "print(decoded_dataset[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets make an embedding matrix \n",
    "\n",
    "where each character has its own embedding vector \n",
    "lets make the vectors of depth 1000!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "vocab_length = len(vocab_letters)\n",
    "\n",
    "embedding_matrix = nn.Embedding(vocab_length, 5, sparse=True)\n",
    "\n",
    "# ok so now every single element of vocab has a respective embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the encoded word: [39, 51, 53, 52, 45, 59, 57, 57, 63]\n",
      "this is the word embedding: \n",
      " tensor([[ 2.2615,  1.2927,  0.7182,  0.6097, -0.2294],\n",
      "        [ 0.3377, -0.6146,  2.0111,  0.2768, -0.5767],\n",
      "        [-1.5159, -3.4483, -0.8304, -0.3416,  1.6252],\n",
      "        [ 0.9731, -0.2882, -1.0467, -0.1645,  2.4650],\n",
      "        [ 1.2739, -1.1889, -1.1373, -1.1227,  0.9211],\n",
      "        [ 0.9225, -0.4928,  0.5311, -1.8355,  0.9262],\n",
      "        [ 0.9395, -0.5285, -1.0335, -0.1193, -2.1736],\n",
      "        [ 0.9395, -0.5285, -1.0335, -0.1193, -2.1736],\n",
      "        [ 0.3213,  0.6322,  1.1415,  0.4657, -1.2951]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# quick example \n",
    "\n",
    "# lets tokenize a word and embbed it \n",
    "\n",
    "word = 'amongussy'\n",
    "\n",
    "encoded_word = str_to_int(word)\n",
    "\n",
    "print('this is the encoded word:' ,encoded_word)\n",
    "\n",
    "# get the embeddings for each one of these tokens \n",
    "\n",
    "# turn list into a tensor (basically the same as a list)\n",
    "encoded_word_tensor = torch.LongTensor(encoded_word)\n",
    "\n",
    "embedded_word = embedding_matrix(encoded_word_tensor)\n",
    "\n",
    "print('this is the word embedding: \\n', embedded_word)\n",
    "\n",
    "# basically we are getting the index of each one of those elements and retrieving the embedding from  it\n",
    "# reminded --> the embedding class starts as a random matrix and converges to true, meaning rich emebddings for each word!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets encode the whole damn dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first thousand elements of the dataset  [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59, 1, 39, 56, 43, 1, 39, 50, 50, 1, 56, 43, 57, 53, 50, 60, 43, 42, 1, 56, 39, 58, 46, 43, 56, 1, 58, 53, 1, 42, 47, 43, 1, 58, 46, 39, 52, 1, 58, 53, 1, 44, 39, 51, 47, 57, 46, 12, 0, 0, 13, 50, 50, 10, 0, 30, 43, 57, 53, 50, 60, 43, 42, 8, 1, 56, 43, 57, 53, 50, 60, 43, 42, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 18, 47, 56, 57, 58, 6, 1, 63, 53, 59, 1, 49, 52, 53, 61, 1, 15, 39, 47, 59, 57, 1, 25, 39, 56, 41, 47, 59, 57, 1, 47, 57, 1, 41, 46, 47, 43, 44, 1, 43, 52, 43, 51, 63, 1, 58, 53, 1, 58, 46, 43, 1, 54, 43, 53, 54, 50, 43, 8, 0, 0, 13, 50, 50, 10, 0, 35, 43, 1, 49, 52, 53, 61, 5, 58, 6, 1, 61, 43, 1, 49, 52, 53, 61, 5, 58, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 24, 43, 58, 1, 59, 57, 1, 49, 47, 50, 50, 1, 46, 47, 51, 6, 1, 39, 52, 42, 1, 61, 43, 5, 50, 50, 1, 46, 39, 60, 43, 1, 41, 53, 56, 52, 1, 39, 58, 1, 53, 59, 56, 1, 53, 61, 52, 1, 54, 56, 47, 41, 43, 8, 0, 21, 57, 5, 58, 1, 39, 1, 60, 43, 56, 42, 47, 41, 58, 12, 0, 0, 13, 50, 50, 10, 0, 26, 53, 1, 51, 53, 56, 43, 1, 58, 39, 50, 49, 47, 52, 45, 1, 53, 52, 5, 58, 11, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 1, 42, 53, 52, 43, 10, 1, 39, 61, 39, 63, 6, 1, 39, 61, 39, 63, 2, 0, 0, 31, 43, 41, 53, 52, 42, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 27, 52, 43, 1, 61, 53, 56, 42, 6, 1, 45, 53, 53, 42, 1, 41, 47, 58, 47, 64, 43, 52, 57, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 35, 43, 1, 39, 56, 43, 1, 39, 41, 41, 53, 59, 52, 58, 43, 42, 1, 54, 53, 53, 56, 1, 41, 47, 58, 47, 64, 43, 52, 57, 6, 1, 58, 46, 43, 1, 54, 39, 58, 56, 47, 41, 47, 39, 52, 57, 1, 45, 53, 53, 42, 8, 0, 35, 46, 39, 58, 1, 39, 59, 58, 46, 53, 56, 47, 58, 63, 1, 57, 59, 56, 44, 43, 47, 58, 57, 1, 53, 52, 1, 61, 53, 59, 50, 42, 1, 56, 43, 50, 47, 43, 60, 43, 1, 59, 57, 10, 1, 47, 44, 1, 58, 46, 43, 63, 0, 61, 53, 59, 50, 42, 1, 63, 47, 43, 50, 42, 1, 59, 57, 1, 40, 59, 58, 1, 58, 46, 43, 1, 57, 59, 54, 43, 56, 44, 50, 59, 47, 58, 63, 6, 1, 61, 46, 47, 50, 43, 1, 47, 58, 1, 61, 43, 56, 43, 0, 61, 46, 53, 50, 43, 57, 53, 51, 43, 6, 1, 61, 43, 1, 51, 47, 45, 46, 58, 1, 45, 59, 43, 57, 57, 1, 58, 46, 43, 63, 1, 56, 43, 50, 47, 43, 60, 43, 42, 1, 59, 57, 1, 46, 59, 51, 39, 52, 43, 50, 63, 11, 0, 40, 59, 58, 1, 58, 46, 43, 63, 1, 58, 46, 47, 52, 49, 1, 61, 43, 1, 39, 56, 43, 1, 58, 53, 53, 1, 42, 43, 39, 56, 10, 1, 58, 46, 43, 1, 50, 43, 39, 52, 52, 43, 57, 57, 1, 58, 46, 39, 58, 0, 39, 44, 44, 50, 47, 41, 58, 57, 1, 59, 57, 6, 1, 58, 46, 43, 1, 53, 40, 48, 43, 41, 58, 1, 53, 44, 1, 53, 59, 56, 1, 51, 47, 57, 43, 56, 63, 6, 1, 47, 57, 1, 39, 57, 1, 39, 52, 0, 47, 52, 60, 43, 52, 58, 53, 56, 63, 1, 58, 53, 1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47, 57, 43, 1, 58, 46, 43, 47, 56, 1, 39, 40, 59, 52, 42, 39, 52, 41, 43, 11, 1, 53, 59, 56, 0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43, 1, 47, 57, 1, 39, 1, 45, 39, 47, 52, 1, 58, 53, 1, 58, 46, 43, 51, 1, 24, 43, 58, 1, 59, 57, 1, 56, 43, 60, 43, 52, 45, 43, 1, 58, 46, 47, 57, 1, 61, 47, 58, 46, 0, 53, 59, 56, 1, 54, 47, 49, 43, 57, 6, 1, 43, 56, 43, 1, 61, 43, 1, 40, 43, 41, 53, 51, 43, 1, 56, 39, 49, 43, 57, 10, 1, 44, 53, 56, 1, 58, 46, 43, 1, 45, 53, 42, 57, 1, 49, 52, 53, 61, 1, 21, 0, 57, 54, 43, 39, 49, 1, 58, 46, 47, 57, 1, 47, 52, 1, 46, 59, 52, 45, 43, 56, 1, 44, 53, 56, 1, 40, 56, 43, 39, 42, 6, 1, 52, 53, 58, 1, 47, 52, 1, 58, 46, 47, 56, 57, 58, 1, 44, 53, 56, 1, 56, 43, 60, 43, 52, 45, 43, 8, 0, 0]\n",
      "tensor([18, 47, 56,  ..., 45,  8,  0])\n",
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# lets encode the whole damn dataset \n",
    "\n",
    "encoded_dataset = str_to_int(text)\n",
    "\n",
    "print('first thousand elements of the dataset ',encoded_dataset[:1000])\n",
    "\n",
    "data = torch.tensor(encoded_dataset, dtype=torch.long)\n",
    "\n",
    "print(data)\n",
    "\n",
    "# pprint('this is the encoded dataset \\n', str(encoded_dataset))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like thisb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3145,  0.4172,  1.2170, -1.0035, -0.5640],\n",
       "        [-1.5588,  0.8676,  1.0475,  1.7504,  0.0233],\n",
       "        [-2.0481, -1.9609,  0.2787, -0.2928,  0.0828],\n",
       "        [ 0.9395, -0.5285, -1.0335, -0.1193, -2.1736],\n",
       "        [ 0.2413, -0.3848, -1.4866,  2.8223, -0.7338],\n",
       "        [-0.1004,  0.2456,  0.0935, -0.0538, -0.8991],\n",
       "        [-0.0486, -0.3853,  0.7480,  1.1025, -0.1715],\n",
       "        [-1.5588,  0.8676,  1.0475,  1.7504,  0.0233],\n",
       "        [ 0.2413, -0.3848, -1.4866,  2.8223, -0.7338],\n",
       "        [-1.5588,  0.8676,  1.0475,  1.7504,  0.0233],\n",
       "        [ 0.3477, -2.0663,  0.4960, -0.0407, -0.0199],\n",
       "        [ 0.5980,  1.5773, -1.6572,  0.5171, -0.6409],\n",
       "        [ 0.9731, -0.2882, -1.0467, -0.1645,  2.4650],\n",
       "        [-0.2048,  0.0537, -0.3139,  0.0148,  1.6283],\n",
       "        [-0.8620,  0.7733,  1.4342,  1.5054,  0.6243],\n",
       "        [-0.2675, -0.4354,  1.2403, -0.2514, -1.7759]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets retrieve the embeddings for the first 8 elements of the dataset \n",
    "\n",
    "sequence_len = 16\n",
    "\n",
    "# this pulls sequence_len amount of embeddings out of the matrix from the dataset indexes \n",
    "# the rows are the embeddings and columns are the features \n",
    "embedding_matrix(data[:sequence_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embbed the whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394, 5])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_dataset= embedding_matrix(data)\n",
    "\n",
    "# now each letter has its own embedding vector \n",
    "embedded_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset is embedded!!\n",
    "\n",
    "pass into transformer block\n",
    "\n",
    "what's in the **block**??\n",
    "\n",
    "#### 1. attention head \n",
    "#### 2. feedforward layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we gotta figure out how to do a foward pass\n",
    "- what does the forward pass do??\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch that\n",
    "\n",
    "lets implement a head of attention first, then we can put this into the model\n",
    "- why? bc i felt like it lel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Attention\n",
    "#### What does a head of attention do??\n",
    "\n",
    "it creates \n",
    "1. queries ‚ùì\n",
    "2. keys üîë\n",
    "3. values üó£Ô∏è\n",
    "\n",
    "we multiply k and q to get the affinities/attention scores between embeddings\n",
    "\n",
    "softmax attention scores to get weights\n",
    "\n",
    "mult weights & values to get the ŒîEmbedding\n",
    "\n",
    "add ŒîEmbedding to Embeddings recursively \n",
    "- slowly add more and more semantic meaning to embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Actually jk\n",
    "\n",
    "lets get this thing to create completions.\n",
    "\n",
    "even if they suck, lets just get it to work first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 65])\n",
      "tensor([[-0.1935, -0.6324, -0.2059,  ..., -0.5336, -1.3621,  0.3269],\n",
      "        [ 1.6515, -0.0424, -0.7355,  ...,  0.8682,  2.0593, -0.8159],\n",
      "        [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
      "        ...,\n",
      "        [ 1.1407,  0.8935, -2.4000,  ...,  0.3227,  1.5431, -1.0392],\n",
      "        [ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
      "        [ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#  the bigram learns probabilities of the next token based on the previous one \n",
    "\n",
    "# the model's forward function gets called when you call the model\n",
    "#  this is part of the nn.Module functionality \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size): \n",
    "        super().__init__()\n",
    "        # initialize the embedding table for every element in the vocab\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # get the logits of the last character \n",
    "        # measure how far off we where from our targets\n",
    "        embeddings = self.token_embedding_table(idx) # --> returns indexes + embeddings \n",
    "        # basically (B,T, C) adding the channel, C dimension\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_length)\n",
    "\n",
    "# here, we are embedding the first 1000 elements of the dataset, making them 65-dimensional vectors \n",
    "embedding_tensor = model.forward(data[:1000])\n",
    "\n",
    "print(embedding_tensor.shape)\n",
    "pprint(embedding_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick little recap of wtf we just did \n",
    "\n",
    "so we started a bigram language model \n",
    "\n",
    "we initalized the super() parent module nn.Module \n",
    "\n",
    "we created an embedding table of vocab_size by n-dimensions\n",
    "\n",
    "we created the forward function with limited utility \n",
    "- right now all it does is retrieve the vector embeddings for the indices that are passed into it \n",
    "- returning a B, T, C tensor where T is sequence length and C is the channels of the embeddings \n",
    "\n",
    "now- we dont want to run the entire sequence linearly \n",
    "- we can strip out the sequence into batches \n",
    "    - this allows us to run many batches n parallel\n",
    "    - parallelism is our friend \n",
    "    - we can do many sequences in parallel instead of one linearly \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
